\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}绪 \hskip 1em\relax 论}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}概率分布}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}线性回归模型}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}线性基底函数模型}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:3-1a}{{\caption@xref {fig:3-1a}{ on input line 146}}{7}}
\newlabel{fig:3-1b}{{\caption@xref {fig:3-1b}{ on input line 146}}{7}}
\newlabel{fig:3-1c}{{\caption@xref {fig:3-1c}{ on input line 146}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.1}{\ignorespaces 基底函数示例，从左至右分别为多项式基底函数，高斯基底函数(3.4)和sigmoid基底函数(3.5)。\relax }}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}最大似然与最小二乘法}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}最小二乘法的几何意义}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.2}{\ignorespaces 分量为$t_1,...,t_N$的$N$维空间中最小二乘解的几何解释。通过寻找数据向量$\boldsymbol  {\mathsf  {t}}$在子空间中的正交投影确定最小二乘回归函数，这个子空间是基底函数$\phi _j(\boldsymbol  {\mathrm  {x}})$通过线性组合展开形成的，基底函数$\boldsymbol  {\varphi }_j$为长度为$N$，元素为$\phi _j(\boldsymbol  {\mathrm  {x}}_n)$的向量。\relax }}{10}}
\newlabel{fig:3-2}{{3{}.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}顺序学习}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}正则化最小二乘法}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.3}{\ignorespaces 正则项(3.29)在$q$取不同值时的图像\relax }}{12}}
\newlabel{fig:3-3}{{3{}.3}{12}}
\newlabel{fig:3-4a}{{\caption@xref {fig:3-4a}{ on input line 308}}{13}}
\newlabel{fig:3-4b}{{\caption@xref {fig:3-4b}{ on input line 308}}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.4}{\ignorespaces 未正则化的误差函数(蓝色曲线)与$q=2$(二次正则项，左)和1(lasso正则项，右)时的约束区域(3.30)，参数向量$\boldsymbol  {\mathrm  {w}}$的最优解为$\boldsymbol  {\mathrm  {w}}^{\star }$。lasso正则项可以给出$w_1^{\star }=0$这样的稀疏解。\relax }}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}多项输出}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}偏差-方差分解}{14}}
\newlabel{fig:3-5a}{{\caption@xref {fig:3-5a}{ on input line 439}}{17}}
\newlabel{fig:3-5b}{{\caption@xref {fig:3-5b}{ on input line 439}}{17}}
\newlabel{fig:3-5c}{{\caption@xref {fig:3-5c}{ on input line 439}}{17}}
\newlabel{fig:3-5d}{{\caption@xref {fig:3-5d}{ on input line 439}}{17}}
\newlabel{fig:3-5e}{{\caption@xref {fig:3-5e}{ on input line 439}}{17}}
\newlabel{fig:3-5f}{{\caption@xref {fig:3-5f}{ on input line 439}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.5}{\ignorespaces 偏差和方差与模型复杂度相关，模型的复杂度由参数$\lambda $控制，利用第1章中的正弦函数数据集得到的结果。数据集的数量$L=100$，每个数据集里包含$N=25$组数据，模型中包含24个高斯基底函数，算上偏差参数，参数的总数为$M=25$。左侧一列展示的是在不同$\qopname  \relax o{ln}\lambda $下的模型拟合结果(为了简洁起见并没有把100个拟合结果全画出来，而是只画了20个)。右侧一列展示的是相应产生的平均拟合结果(红色曲线)和真实的正弦函数曲线(绿色曲线)。\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.6}{\ignorespaces 平方偏差、方差及其总和的图像，这个结果是对应图3.5的。同时也展示了一个包含1000个数据的测试集的平均误差。平方偏差+方差会在$\qopname  \relax o{ln}\lambda = -0.31$附近取得最小值，非常接近测试数据集上取得最小误差的位置。\relax }}{18}}
\newlabel{fig:3-6}{{3{}.6}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}贝叶斯线性回归}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}参数分布}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.7}{\ignorespaces 对于线性模型$y(x,\boldsymbol  {\mathrm  {w}})=w_0+w_1 x$进行顺序贝叶斯学习的示意图。每个图像对应的具体含义请详见文字部分。\relax }}{21}}
\newlabel{fig:3-7}{{3{}.7}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}预测分布}{22}}
\newlabel{fig:3-8a}{{\caption@xref {fig:3-8a}{ on input line 555}}{23}}
\newlabel{fig:3-8b}{{\caption@xref {fig:3-8b}{ on input line 555}}{23}}
\newlabel{fig:3-8c}{{\caption@xref {fig:3-8c}{ on input line 555}}{23}}
\newlabel{fig:3-8d}{{\caption@xref {fig:3-8d}{ on input line 555}}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.8}{\ignorespaces 根据第1.1节中的正弦函数数据集和一个包含9个高斯基底函数(3.4)的模型构建的预测分布(3.58)。具体的讨论详见正文。\relax }}{23}}
\newlabel{fig:3-9a}{{3{}.8}{23}}
\newlabel{fig:3-9b}{{3{}.8}{23}}
\newlabel{fig:3-9c}{{3{}.8}{23}}
\newlabel{fig:3-9d}{{3{}.8}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.9}{\ignorespaces 从图3.8所示的分布中抽取$\boldsymbol  {\mathrm  {w}}$后画出的对应函数$y(x,\boldsymbol  {\mathrm  {w}})$的图像。\relax }}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}等价核}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.10}{\ignorespaces 图3.1中的高斯基底函数的等价核$k(x,x')$，在这里画成了$x-x'$的形式，同时给出了该矩阵的3个不同$x$取值处的具体情况。生成这个核所用的数据$x$是从$(-1,1)$之间均匀取出的200个值。\relax }}{25}}
\newlabel{fig:3-10}{{3{}.10}{25}}
\newlabel{fig:3-11a}{{\caption@xref {fig:3-11a}{ on input line 613}}{25}}
\newlabel{fig:3-11b}{{\caption@xref {fig:3-11b}{ on input line 613}}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.11}{\ignorespaces $x=0$时的等价核，这里画成了$x$的函数的形式，左侧为多项式基底函数，右侧为sigmoid基底函数。需要注意的是，尽管它们对应的基底函数并非是局部的，它们也是关于$x$的局部函数。\relax }}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}贝叶斯模型的对比}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.12}{\ignorespaces 如果我们假设参数的后验分布是在其模$w_{\mathrm  {MAP}}$附近的尖峰，可以获得如图所示的模型证据粗略近似。\relax }}{28}}
\newlabel{fig:3-12}{{3{}.12}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.13}{\ignorespaces 三种不同复杂度的模型各自的数据集分布，$\mathcal  {M}_1$是复杂度最低的，$\mathcal  {M}_3$的复杂度最高的。这些分布都是经过归一化的。在这个例子中，对于某个数据集$\mathcal  {D}_0$，中等复杂的模型$\mathcal  {M}_2$给出的模型证据最大。\relax }}{29}}
\newlabel{fig:3-13}{{3{}.13}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}证据近似}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}证据函数的评估}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.14}{\ignorespaces 关于阶数$M$绘制的模型对数证据的图像，对于多项式回归模型而言，模型证据最倾向于选择$M=3$的模型。\relax }}{33}}
\newlabel{fig:3-14}{{3{}.14}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}证据函数的最大化}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}有效参数的数量}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.15}{\ignorespaces 似然函数的轮廓线(红色曲线)和先验概率分布(绿色曲线)，参数空间的坐标轴经过旋转后与Hessian矩阵的特征向量$\mathbf  {u}_i$对齐。对于$\alpha =0$，后验的模为最大似然解$\boldsymbol  {\mathrm  {w}}_{\mathrm  {ML}}$，而对于$\alpha \not =0$，模则位于$\boldsymbol  {\mathrm  {w}}_{\mathrm  {MAP}}=\mathbf  {m}_N$。在方向$w_1$上，由(3.87)定义的特征值$\lambda $比$\alpha $小，因此$\lambda _1 / (\lambda _1+\alpha )$接近于0，于是$w_1$的MAP值也接近于0。与之相反，在$w_2$的方向上，特征值$\lambda _2$大于$\alpha $，因此 $\lambda _2 / (\lambda _2+\alpha )$接近于1，$w_2$的MAP值接近于最大似然值。\relax }}{35}}
\newlabel{fig:3-15}{{3{}.15}{35}}
\newlabel{fig:3-16a}{{\caption@xref {fig:3-16a}{ on input line 851}}{36}}
\newlabel{fig:3-16b}{{\caption@xref {fig:3-16b}{ on input line 851}}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.16}{\ignorespaces 左图为$\gamma $和$2\alpha E_W(\mathbf  {m}_N)$与$\qopname  \relax o{ln}\alpha $的函数关系(分别为红色曲线和蓝色曲线)，这里的数据集是正弦函数数据集。这两条曲线的交点正是最优解$\alpha $。右图中的红色曲线是对数模型证据$\qopname  \relax o{ln}p(\boldsymbol  {\mathsf  {t}}|\alpha ,\beta )$关于$\qopname  \relax o{ln}\alpha $的函数关系，恰好在左图中的交点处取得峰值。蓝色曲线为测试集误差，验证了在模型证据取得最大值时模型具有最优的泛化能力。\relax }}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {3{}.17}{\ignorespaces 高斯基底函数模型中10个参数$w_i$与有效参数数量$\gamma $的关系，其中超参数$0 \leqslant \alpha \leqslant \infty $，于是$\gamma $的范围是$0 \leqslant \gamma \leq M$。\relax }}{36}}
\newlabel{fig:3-17}{{3{}.17}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}固定基底函数的局限性}{37}}
